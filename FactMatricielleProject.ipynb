{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FactMatricielleProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_j5izu0fcst"
      },
      "source": [
        "\n",
        "# **Projet Factorisation Matricielle** \n",
        "\n",
        "> Implémentation de Baseline Estimates et SVD++\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMU9HF5kgAeJ"
      },
      "source": [
        "## Lecture des fichiers à partir du Google Drive                     (Authentification puis Téléchargement des données)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFIxwkd8Ko5z"
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFgExMlxKwAF"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TelJUlRAK0uA"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt9k7f-BLMEz"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1ZNkUgjXSTmJk773T876mRRxBb8NJz4n8\"})   # The id of the file ratings file\n",
        "downloaded.GetContentFile('ratings.csv')        # The name of the file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1AVLQIgjfwphguD2Zk4CYg6mZx1jQUJWP\"})   # The id of the file favorites file\n",
        "downloaded.GetContentFile('favorites.csv')        # The name of the file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1nhHkhSwS4-an-CRrnXbPY4TzLDQcUNA1\"})   # The id of the file bookmarks file\n",
        "downloaded.GetContentFile('bookmarks.csv')        # The name of the file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1SVbczf812-0krGTuaIFyw68MvXbAMgUt\"})   # The id of the file bookmarks file\n",
        "downloaded.GetContentFile('bookmarks_idx_train.npy')        # The name of the file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1ystO6DcVsiovAWr_JmgsulYEDKIj_-WG\"})   # The id of the file bookmarks file\n",
        "downloaded.GetContentFile('bookmarks_idx_test.npy')        # The name of the file\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY3dRm3DgPxf"
      },
      "source": [
        "## Implémentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve8TNRoKgVhO"
      },
      "source": [
        "Dans tout ce qui suit, nous avons travaillé seulement avec les deux bibliothèques **Numpy** et **Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmGFk5hiokam"
      },
      "source": [
        "df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSHX2X0aglI5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Data_Description() :** La fonction responsable de la lecture des données, la génération de **Interest Dataset** et la génération du **Training** & **Test** Datasets. \n",
        "\n",
        "\n",
        "---\n",
        "Pour la génération du Interest Dataset, la formule utilisée est la suivante :   \n",
        "$$\n",
        "  r_{ui} = w_{ui} + n_{ui} + f_{ui}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4XJ0ifJZIoV"
      },
      "source": [
        "def Data_Description():\n",
        "    \n",
        "    first = True\n",
        "    Data = pd.DataFrame({'id_profile':[], 'id_asset':[],'interest':[] })\n",
        "    ratings = pd.read_csv(\"ratings.csv\")\n",
        "    favorites = pd.read_csv(\"favorites.csv\")\n",
        "    indexes_test = np.load('bookmarks_idx_test.npy')\n",
        "    indexes_train = np.load('bookmarks_idx_train.npy')\n",
        "    \n",
        "    ratings.set_index(['id_profile','id_asset'],inplace=True)\n",
        "    favorites.set_index(['id_profile','id_asset'],inplace=True)\n",
        "    i=1    \n",
        "    for bookmarks in pd.read_csv(\"bookmarks.csv\",chunksize=1000000):\n",
        "        i+=1\n",
        "        D = pd.DataFrame()\n",
        "        D['id_profile']=bookmarks.id_profile\n",
        "        D['id_asset']=bookmarks.id_asset\n",
        "        bookmarks[\"interest\"]=1\n",
        "        bookmarks.set_index(['id_profile','id_asset'],inplace=True)\n",
        "        \n",
        "        bfindex= set(bookmarks.index) & set(favorites.index)\n",
        "        brindex = set(bookmarks.index) & set(ratings.index)\n",
        "        bookmarks.loc[bfindex,'interest'] += 5\n",
        "        try:\n",
        "          bookmarks.loc[brindex,'interest'] += ratings.loc[set(brindex)]['score']\n",
        "        except:\n",
        "          \n",
        "          df = bookmarks.loc[brindex]\n",
        "          indexes = df[df.index.duplicated(keep='first')].index\n",
        "          ratings_dup = ratings\n",
        "          for elm in indexes:\n",
        "            ratings_dup = ratings_dup.append(ratings.loc[elm])\n",
        "          score = np.array(ratings_dup.loc[brindex].score)\n",
        "          bookmarks.loc[brindex,'interest'] += score\n",
        "\n",
        "        D['interest'] = np.array(bookmarks['interest'])\n",
        "        Data =pd.concat([Data,D])\n",
        "    Trainset = Data.loc[indexes_train]\n",
        "    Testset  = Data.loc[indexes_test]\n",
        "\n",
        "    return Trainset,Testset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9STYm6j5hR_5"
      },
      "source": [
        "**Create_bui() :** La fonction responsable de la création des matrices \n",
        "$$ B_{ui} $$    et  $$ R_{ui} $$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQVVJw78O54L"
      },
      "source": [
        "import numpy as np\n",
        "import decimal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_bui(Trainset,Bu,Bi,mu,UsersDic,MoviesDic):\n",
        "  \n",
        "  Rui = np.zeros([len(Trainset.id_profile.unique()),len(Trainset.id_asset.unique())])\n",
        "  Bui = np.zeros([len(Trainset.id_profile.unique()),len(Trainset.id_asset.unique())])\n",
        "  \n",
        "  buIndex = np.zeros(len(Trainset.id_profile.unique()),dtype=int)\n",
        "  biIndex = np.zeros(len(Trainset.id_asset.unique()),dtype=int)\n",
        "\n",
        "  Users_Key = list(Trainset.id_profile.unique().astype(int))\n",
        "  Users_value = range(len(Users_Key))\n",
        "  zip_iterator = zip(Users_Key, Users_value)\n",
        "  j_index = dict(zip_iterator) \n",
        "\n",
        "  Movies_Key = list(Trainset.id_asset.unique())\n",
        "  Movies_value = range(len(Movies_Key))\n",
        "  zip_iterator = zip(Movies_Key, Movies_value)\n",
        "  k_index = dict(zip_iterator) \n",
        "\n",
        "  \n",
        "  \n",
        "  Trainset.set_index(['id_profile','id_asset'],inplace=True)\n",
        "\n",
        "  for (u,i), elm in Trainset.iterrows():\n",
        "    U = UsersDic[u]\n",
        "    I = MoviesDic[i]\n",
        "    j = j_index[u]\n",
        "    k = k_index[i]\n",
        "    Rui[j,k] = elm['interest']\n",
        "    Bui[j,k] = mu + Bu[U] + Bi[I] \n",
        "    buIndex[j]= U \n",
        "    biIndex[k]= I\n",
        "\n",
        "  return Rui,Bui,buIndex,biIndex\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEekLliD7166"
      },
      "source": [
        "**gradient_J() :**  La fonction qui calcule les dérivés par rapport Bu et Bi. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L3_6Ak955At"
      },
      "source": [
        "def gradient_J(Rui,Bui,Bu,Bi,BuI,BiI) :\n",
        "\n",
        "  bJu = 2*np.apply_along_axis( np.sum, 1 , Bui-Rui ) + ( (2/512) * 0.002 * Bu[BuI] )\n",
        "\n",
        "  bJi = 2* np.apply_along_axis(np.sum, 1, np.transpose(Bui-Rui)) + ( (2/512) * 0.002 * Bi[BiI])\n",
        "  \n",
        "  return bJu,bJi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDeIX5U3jVk3"
      },
      "source": [
        "**mse() :** calcule la RMSE, qui represente le taux d'erreur quadratique, qui a pour but d'évaluer la performance des résultats obtenus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvg1-Y6LuhxW"
      },
      "source": [
        "def mse( Testset,Trainset,Bu,Bi,mu ) :\n",
        "  df_Bi = pd.DataFrame()\n",
        "  df_Bi['Bi'] = Bi\n",
        "  df_Bi['id_asset'] = Trainset.id_asset.unique()\n",
        "  df_Bu = pd.DataFrame()\n",
        "  df_Bu['Bu'] = Bu\n",
        "  df_Bu['id_profile'] = Trainset.id_profile.unique()\n",
        "  Testset = Testset.merge(df_Bi,on= 'id_asset')\n",
        "  Testset = Testset.merge(df_Bu,on= 'id_profile')\n",
        "  Testset['Rui'] = Testset['Bu'] + Testset['Bi'] + mu \n",
        "  X = Testset.Rui.values\n",
        "  Y = Testset.interest.values  \n",
        "  return (1/len(X)) * np.sum(np.square(Y - X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx9aUOb9jmWx"
      },
      "source": [
        "**L'algorithme de la descente du gradient stochastique**  est présenté dans la fonction suivante, qui prend en paramètre le Traintest et Testset, et utilise la technique des mini batchs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZWP8yPfnEYY"
      },
      "source": [
        "def gradient_descent(Trainset,Testset):\n",
        "\n",
        "  Users_Key = list(Trainset.id_profile.unique().astype(int))\n",
        "\n",
        "  Users_value = range(len(Users_Key))\n",
        "  zip_iterator = zip(Users_Key, Users_value)\n",
        "  UsersDic = dict(zip_iterator) \n",
        "\n",
        "  Movies_Key = list(Trainset.id_asset.unique())\n",
        "  Movies_value = range(len(Movies_Key))\n",
        "  zip_iterator = zip(Movies_Key, Movies_value)\n",
        "  MoviesDic = dict(zip_iterator) \n",
        "\n",
        "  Bu = np.zeros(len(Users_Key))\n",
        "  Bi = np.zeros(len(Movies_Key))\n",
        "\n",
        "  average = np.average(Trainset.interest)\n",
        "\n",
        "  max_iter = 10\n",
        "  nminibatchlen = int(len(Trainset)/512)\n",
        "  Trainset.reset_index(inplace=True)\n",
        "  for i in range(max_iter):\n",
        "    begin = 0\n",
        "    end = 511\n",
        "    print('Mse = {}'.format(mse(Testset,Trainset,Bu,Bi,average)) )\n",
        "\n",
        "    for j in range(nminibatchlen):\n",
        "      Rui,Bui,BuI,BiI = create_bui(Trainset.loc[begin:end],Bu,Bi,average,UsersDic,MoviesDic)\n",
        "      Bju,Bji= gradient_J(Rui,Bui,Bu,Bi,BuI,BiI)\n",
        "      Bu[BuI] -= 0.001*Bju\n",
        "      Bi[BiI] -= 0.001*Bji\n",
        "      begin +=512\n",
        "      end   +=512\n",
        "        \n",
        "\n",
        "    if(begin < len(Trainset)):\n",
        "      Rui,Bui,BuI,BiI = create_bui(Trainset.loc[begin:],Bu,Bi,3.2,UsersDic,MoviesDic)\n",
        "      Bju,Bji= gradient_J(Rui,Bui,Bu,Bi,BuI,BiI)\n",
        "      Bu[BuI] -= 0.001*Bju\n",
        "      Bi[BiI] -= 0.001*Bji\n",
        "  \n",
        "  return Bu,Bi\n",
        "      \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1mmja-xmg2A"
      },
      "source": [
        "On extrait le Trainset et Testset a l'aide de la fonction Data_Description(). Tout les valeurs dupliquées en fonction de l'index (\"id_profile\",\"id_asset\") seront également supprimées.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laQMq-6sLLxY"
      },
      "source": [
        "Trainset,Testset = Data_Description()\n",
        "Trainset = Trainset[~Trainset.index.duplicated(keep='first')]\n",
        "Testset  = Testset[~Testset.index.duplicated(keep='first')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlsoYqGnnFcG"
      },
      "source": [
        "On exécute l'algorithme de la descente du gradient stochastique, on obtient comme résultat Bu et Bi, on peut également tester la performance de notre modèle avec la fonction mse() qui est déjà appelé dans la fonction graient_escent() à chaque itération du TestSet (un epoch) on affiche mse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOhVU_LUDHx6"
      },
      "source": [
        "Bu,Bi = gradient_descent(Trainset,Testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cicv5NL64N9I"
      },
      "source": [
        "### **SVD++**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMK8DX4v_O6W"
      },
      "source": [
        "Dans cette partie, nous avons implémenté la méthode SVD++ de Koren "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD2K7SIO4UoM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numFactors =  20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoQQvx6rBnFD"
      },
      "source": [
        "Dans cette fonction on crée deux matrices de training et de test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLAjx_4sBMXE"
      },
      "source": [
        "def train_test_split(n_users,n_items):\n",
        "    \n",
        "    Trainset,Testset = Data_Description()\n",
        "    train_data = Trainset[~Trainset.index.duplicated(keep='first')]\n",
        "    test_data  = Testset[~Testset.index.duplicated(keep='first')]\n",
        "\n",
        "    R = np.zeros((n_users, n_items))\n",
        "    length_train = len(train_data)\n",
        "    length_test = len(test_data)\n",
        "    sum_train = 0\n",
        "    sum_test = 0\n",
        "    item_by_users = {}\n",
        "    users_by_item = {}\n",
        "    rate_by_users = {}\n",
        "    for line in train_data.itertuples():\n",
        "        R[line[1] - 1, line[2] - 1] = line[3]\n",
        "        sum_train += line[3]\n",
        "        item_by_users.setdefault(line[1]-1, []).append(line[2]-1)\n",
        "        users_by_item.setdefault(line[2]-1, []).append(line[1]-1)\n",
        "        rate_by_users.setdefault(line[1]-1, []).append(line[3])\n",
        "    average_train = float(sum_train/length_train)\n",
        "    T = np.zeros((n_users, n_items))\n",
        "    for line in test_data.itertuples():\n",
        "        T[line[1] - 1, line[2] - 1] = line[3]\n",
        "        sum_test += line[3]\n",
        "    average_test = float(sum_test/length_test)\n",
        "    return R, T, average_train,average_test,item_by_users,users_by_item,rate_by_users\n",
        "\n",
        "# Index matrix for training data\n",
        "def index_matrix(R,T):\n",
        "    I = R.copy()\n",
        "    I[I > 0] = 1\n",
        "    I[I == 0] = 0\n",
        "    I2 = T.copy()\n",
        "    I2[I2 > 0] = 1\n",
        "    I2[I2 == 0] = 0\n",
        "    return I, I2\n",
        "\n",
        "def prediction(P,Q):\n",
        "    return np.dot(P.T,Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izdfITKEDL8E"
      },
      "source": [
        "\n",
        "\n",
        "1.   **mse() :** calcule la RMSE, qui represente le taux d'erreur quadratique, qui a pour but d'évaluer la performance des résultats obtenus.\n",
        "2.   **plot_rmse() :** afficher l'erreur selon le nombre d'epochs\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyKCGCRHBM80"
      },
      "source": [
        "def rmse(I, R, item_by_users,average, Q, P, Y, B_U, B_I):\n",
        "    users, items = R.nonzero()\n",
        "    sum = 0\n",
        "    for u, i in zip(users, items):  # 75000 times\n",
        "        n_u = len(users[users == u])\n",
        "        if (R[u, i] > 5 or R[u, i] < 0):\n",
        "            print(\"R[\" + str(u) + \",\" + str(i) + \"]=\" + R[u, i])\n",
        "        # print(n_u)\n",
        "        pPlusY = np.zeros(numFactors)\n",
        "        for j in item_by_users[u]:\n",
        "            pPlusY = np.add(pPlusY, Y[j, :])\n",
        "        # print(pPlusY)\n",
        "        pPlusY = np.add(pPlusY / np.sqrt(n_u), P[:, u])\n",
        "        error = R[u, i] - (average + B_U[u] + B_I[i] + prediction(pPlusY, Q[:, i]))\n",
        "        sum += error**2\n",
        "    return np.sqrt(sum/len(R[R > 0]))\n",
        "\n",
        "def plotRMSE(n_epochs,train_errors,test_errors):\n",
        "    plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data')\n",
        "    plt.plot(range(n_epochs), test_errors, marker='v', label='Test_data')\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4PSmPHfGgJ7"
      },
      "source": [
        "**svdpp() :** est la fonction SVD++ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qXksR0ABNL8"
      },
      "source": [
        "def svdpp():\n",
        "    #df, n_users, n_items = loadData(path)\n",
        "    ratings = pd.read_csv(\"ratings.csv\")\n",
        "    n_users = ratings['id_profile'].unique().shape[0]\n",
        "    n_items = ratings['id_asset'].unique().shape[0]\n",
        "\n",
        "    R, T, average_train, average_test, item_by_users, users_by_item, rate_by_users= train_test_split(n_users, n_items)\n",
        "    I, I2 = index_matrix(R,T)\n",
        "\n",
        "    gama1 = 0.01\n",
        "    gama2 = 0.01\n",
        "    lambda6 = 0.05\n",
        "    lambda7 = 0.1\n",
        "    num_epochs = 20\n",
        "\n",
        "    m, n = R.shape\n",
        "    users, items = R.nonzero()\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "\n",
        "    P = np.random.rand(numFactors,m) \n",
        "    Q = np.random.rand(numFactors,n) \n",
        "    Y = np.random.rand(n,numFactors) \n",
        "    B_U =  np.random.rand(m)\n",
        "    B_I =  np.random.rand(n)\n",
        "\n",
        "    pPlusY = {}\n",
        "\n",
        "    for u in range(m):\n",
        "        p = np.zeros(numFactors)\n",
        "        for j in item_by_users[u]:\n",
        "            p = np.add(p,Y[j,:])\n",
        "        pPlusY[u] = p\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print(\"epoch=\" + str(epoch))\n",
        "        count = 0\n",
        "        for u, i in zip(users, items):  \n",
        "            n_u = len(users[users == u])\n",
        "            pPlusY[u] = np.add(pPlusY[u] / np.sqrt(n_u), P[:, u])\n",
        "            error = R[u, i] - (average_train + B_U[u]+B_I[i]+prediction( pPlusY[u], Q[:, i]))\n",
        "            print(\"Error=\"+str(error))\n",
        "            P[:, u] += gama2 * (error * Q[:, i] - lambda7 * P[:, u])\n",
        "            Q[:, i] += gama2 * (error * (P[:, u]+ 1 / np.sqrt(n_u) *   pPlusY[u]) - lambda7 * Q[:, i])\n",
        "\n",
        "            for item in item_by_users[u]:\n",
        "                Y[item, :] += gama2 * (error * 1 / np.sqrt(n_u) * Q[:, item] - lambda7 * Y[item,:])\n",
        "            B_U[u] += gama1 * (error - lambda6 * B_U[u])\n",
        "            B_I[i] += gama1 * (error - lambda6 * B_I[i])\n",
        "            count += 1\n",
        "\n",
        "        train_rmse = rmse(I, R, item_by_users,average_train, Q, P, Y, B_U, B_I)\n",
        "        print(\"train_rmse=\"+str(train_rmse))\n",
        "        test_rmse = rmse(I2, T, item_by_users,average_train, Q, P, Y, B_U, B_I)\n",
        "        print(\"test_rmse=\" + str(test_rmse))\n",
        "        train_errors.append(train_rmse)\n",
        "        test_errors.append(test_rmse)\n",
        "\n",
        "    print(\"train_errors=\" + str(train_errors))\n",
        "    print(\"test_errors=\" + str(test_errors))\n",
        "    plotRMSE(num_epochs, train_errors, test_errors)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    svdpp()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}